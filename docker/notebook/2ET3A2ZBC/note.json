{
  "paragraphs": [
    {
      "title": "Reload the GoodReads Books DataSet",
      "text": "%spark\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport spark.implicits._\n\nval booksDDL: String \u003d \"`bookID` INT,`title` STRING,`authors` STRING,`average_rating` STRING,`isbn` STRING,`isbn13` STRING,`language_code` STRING,`num_pages` STRING,`ratings_count` INT,`text_reviews_count` INT\"\nval booksSchema \u003d StructType.fromDDL(booksDDL)\nval df \u003d spark.read\n    .format(\"csv\")\n    .option(\"delimiter\",\",\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"false\")\n    .schema(booksSchema)\n    .load(\"file:///odsc/books.csv\")\n    .toDF(\"bookID\",\"title\",\"authors\",\"average_rating\",\"isbn\",\"isbn13\",\"language_code\",\"num_pages\",\"ratings_count\",\"text_reviews_count\")\ndf.createOrReplaceTempView(\"books\")",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:12.194",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport spark.implicits._\nbooksDDL: String \u003d `bookID` INT,`title` STRING,`authors` STRING,`average_rating` STRING,`isbn` STRING,`isbn13` STRING,`language_code` STRING,`num_pages` STRING,`ratings_count` INT,`text_reviews_count` INT\nbooksSchema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(bookID,IntegerType,true), StructField(title,StringType,true), StructField(authors,StringType,true), StructField(average_rating,StringType,true), StructField(isbn,StringType,true), StructField(isbn13,StringType,true), StructField(language_code,StringType,true), StructField(num_pages,StringType,true), StructField(ratings_count,IntegerType,true), StructField(text_reviews_count,IntegerT..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572296097837_-1049566451",
      "id": "20191028-205457_2007953720",
      "dateCreated": "2019-10-28 20:54:57.837",
      "dateStarted": "2019-10-29 23:50:12.258",
      "dateFinished": "2019-10-29 23:50:13.298",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Simplify finding Missing Values",
      "text": "%spark\ndef missingValues(df: DataFrame): DataFrame \u003d {\n    df.schema.map {\n        case StructField(name: String, _: StringType, _, _) \u003d\u003e\n            // will do isNull orEmpty check here - and count as missing values\n            (name, df.where(col(name).isNull.or(col(name).equalTo(\"\"))).count())\n        case StructField(name: String, _, _, _) \u003d\u003e\n            (name, df.where(col(name).isNull).count)\n    }.toDF(\"name\", \"missing\")\n}",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:13.357",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "missingValues: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572297478115_735095780",
      "id": "20191028-211758_1183307709",
      "dateCreated": "2019-10-28 21:17:58.115",
      "dateStarted": "2019-10-29 23:50:13.441",
      "dateFinished": "2019-10-29 23:50:15.046",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nval missingValuesDF \u003d df.transform(missingValues)\nmissingValuesDF.show(true)",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:15.049",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+-------+\n|              name|missing|\n+------------------+-------+\n|            bookID|      0|\n|             title|      0|\n|           authors|      0|\n|    average_rating|      0|\n|              isbn|      0|\n|            isbn13|      0|\n|     language_code|      0|\n|         num_pages|      0|\n|     ratings_count|      0|\n|text_reviews_count|      0|\n+------------------+-------+\n\nmissingValuesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [name: string, missing: bigint]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://zeppelin:4040/jobs/job?id\u003d3",
            "http://zeppelin:4040/jobs/job?id\u003d4",
            "http://zeppelin:4040/jobs/job?id\u003d5",
            "http://zeppelin:4040/jobs/job?id\u003d6",
            "http://zeppelin:4040/jobs/job?id\u003d7",
            "http://zeppelin:4040/jobs/job?id\u003d8",
            "http://zeppelin:4040/jobs/job?id\u003d9",
            "http://zeppelin:4040/jobs/job?id\u003d10",
            "http://zeppelin:4040/jobs/job?id\u003d11",
            "http://zeppelin:4040/jobs/job?id\u003d12"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1572297516087_1568892013",
      "id": "20191028-211836_954293158",
      "dateCreated": "2019-10-28 21:18:36.088",
      "dateStarted": "2019-10-29 23:50:15.185",
      "dateFinished": "2019-10-29 23:50:18.391",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Simplify finding Distinct Values",
      "text": "%spark\ndef distinctColumnValues(df: DataFrame): DataFrame \u003d {\n    df.schema.map { r \u003d\u003e\n      (r.name, df.where(col(r.name).isNotNull)\n        .select(col(r.name))\n        .distinct()\n        .count())\n    }.toDF(\"name\", \"distinct\")\n}",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:18.467",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "distinctColumnValues: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572297562783_-1718513928",
      "id": "20191028-211922_657279969",
      "dateCreated": "2019-10-28 21:19:22.784",
      "dateStarted": "2019-10-29 23:50:18.534",
      "dateFinished": "2019-10-29 23:50:19.122",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nval distinctValuesDF \u003d df.transform(distinctColumnValues)\ndistinctValuesDF.show(true)",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:19.135",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+--------+\n|              name|distinct|\n+------------------+--------+\n|            bookID|   13719|\n|             title|   12427|\n|           authors|    7605|\n|    average_rating|     221|\n|              isbn|   13719|\n|            isbn13|   13719|\n|     language_code|      35|\n|         num_pages|    1092|\n|     ratings_count|    6030|\n|text_reviews_count|    2025|\n+------------------+--------+\n\ndistinctValuesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [name: string, distinct: bigint]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://zeppelin:4040/jobs/job?id\u003d13",
            "http://zeppelin:4040/jobs/job?id\u003d14",
            "http://zeppelin:4040/jobs/job?id\u003d15",
            "http://zeppelin:4040/jobs/job?id\u003d16",
            "http://zeppelin:4040/jobs/job?id\u003d17",
            "http://zeppelin:4040/jobs/job?id\u003d18",
            "http://zeppelin:4040/jobs/job?id\u003d19",
            "http://zeppelin:4040/jobs/job?id\u003d20",
            "http://zeppelin:4040/jobs/job?id\u003d21",
            "http://zeppelin:4040/jobs/job?id\u003d22"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1572297614798_-2012806089",
      "id": "20191028-212014_1541440365",
      "dateCreated": "2019-10-28 21:20:14.798",
      "dateStarted": "2019-10-29 23:50:19.289",
      "dateFinished": "2019-10-29 23:50:37.061",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\ndef columnStats(df: DataFrame) \u003d {\n    df.schema.map { r \u003d\u003e\n     val colStats \u003d df.select(col(r.name)).describe()\n     /*val colStats \u003d df.select(col(r.name)).where(col(r.name).isNotNull).describe()*/\n     colStats.show()\n    }\n}",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:37.124",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "columnStats: (df: org.apache.spark.sql.DataFrame)Seq[Unit]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572297649294_1217489344",
      "id": "20191028-212049_1743638308",
      "dateCreated": "2019-10-28 21:20:49.295",
      "dateStarted": "2019-10-29 23:50:37.177",
      "dateFinished": "2019-10-29 23:50:37.491",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\ncolumnStats(df)",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:37.579",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------+------------------+\n|summary|            bookID|\n+-------+------------------+\n|  count|             13719|\n|   mean|22160.579269626065|\n| stddev|13699.603351218657|\n|    min|                 1|\n|    max|             47709|\n+-------+------------------+\n\n+-------+--------------------+\n|summary|               title|\n+-------+--------------------+\n|  count|               13719|\n|   mean|              1900.8|\n| stddev|  113.92629196107458|\n|    min|  said the shotgu...|\n|    max|魔戒首部曲：魔戒現身|\n+-------+--------------------+\n\n+-------+--------------------+\n|summary|             authors|\n+-------+--------------------+\n|  count|               13719|\n|   mean|                null|\n| stddev|                null|\n|    min|A.B. Yehoshua-Hil...|\n|    max|Éric-Emmanuel Sch...|\n+-------+--------------------+\n\n+-------+--------------------+\n|summary|      average_rating|\n+-------+--------------------+\n|  count|               13719|\n|   mean|  3.9306198045792398|\n| stddev|   0.357893063472285|\n|    min| Jr.-C.S. Lewis-P...|\n|    max|                5.00|\n+-------+--------------------+\n\n+-------+--------------------+\n|summary|                isbn|\n+-------+--------------------+\n|  count|               13719|\n|   mean| 1.039560314259837E9|\n| stddev|1.5971389842051237E9|\n|    min|                0.00|\n|    max|          9998691567|\n+-------+--------------------+\n\n+-------+--------------------+\n|summary|              isbn13|\n+-------+--------------------+\n|  count|               13719|\n|   mean|9.761170523550744E12|\n| stddev|4.321517728363313E11|\n|    min|       0008987059752|\n|    max|       9790007672386|\n+-------+--------------------+\n\n+-------+-------------------+\n|summary|      language_code|\n+-------+-------------------+\n|  count|              13719|\n|   mean| 9.7813082527492E12|\n| stddev|5.142332110626742E8|\n|    min|      9780674842113|\n|    max|                zho|\n+-------+-------------------+\n\n+-------+------------------+\n|summary|         num_pages|\n+-------+------------------+\n|  count|             13719|\n|   mean| 342.4027271401487|\n| stddev|252.65016483845298|\n|    min|                 0|\n|    max|               eng|\n+-------+------------------+\n\n+-------+------------------+\n|summary|     ratings_count|\n+-------+------------------+\n|  count|             13719|\n|   mean| 17759.02529338873|\n| stddev|112937.13120992463|\n|    min|                 0|\n|    max|           5629932|\n+-------+------------------+\n\n+-------+------------------+\n|summary|text_reviews_count|\n+-------+------------------+\n|  count|             13719|\n|   mean| 533.6063853050514|\n| stddev| 2528.600315867432|\n|    min|                 0|\n|    max|             93619|\n+-------+------------------+\n\nres5: Seq[Unit] \u003d List((), (), (), (), (), (), (), (), (), ())\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://zeppelin:4040/jobs/job?id\u003d23",
            "http://zeppelin:4040/jobs/job?id\u003d24",
            "http://zeppelin:4040/jobs/job?id\u003d25",
            "http://zeppelin:4040/jobs/job?id\u003d26",
            "http://zeppelin:4040/jobs/job?id\u003d27",
            "http://zeppelin:4040/jobs/job?id\u003d28",
            "http://zeppelin:4040/jobs/job?id\u003d29",
            "http://zeppelin:4040/jobs/job?id\u003d30",
            "http://zeppelin:4040/jobs/job?id\u003d31",
            "http://zeppelin:4040/jobs/job?id\u003d32"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1572297748464_-325720515",
      "id": "20191028-212228_655299891",
      "dateCreated": "2019-10-28 21:22:28.464",
      "dateStarted": "2019-10-29 23:50:37.611",
      "dateFinished": "2019-10-29 23:50:40.422",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Data Cleansing: Find where the CSV is broken.\n**hint**: Use `df.limit(1000)` as a starting point. And Check the `average_rating` `max` value",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:40.426",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eData Cleansing: Find where the CSV is broken.\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ehint\u003c/strong\u003e: Use \u003ccode\u003edf.limit(1000)\u003c/code\u003e as a starting point. And Check the \u003ccode\u003eaverage_rating\u003c/code\u003e \u003ccode\u003emax\u003c/code\u003e value\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572298151885_397449802",
      "id": "20191028-212911_18497608",
      "dateCreated": "2019-10-28 21:29:11.886",
      "dateStarted": "2019-10-29 23:50:40.528",
      "dateFinished": "2019-10-29 23:50:40.549",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\ncolumnStats(df.select(\"title\").limit(6000))",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:40.592",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------+--------------------+\n|summary|               title|\n+-------+--------------------+\n|  count|                6000|\n|   mean|              1880.0|\n| stddev|  120.08885599144219|\n|    min|  said the shotgu...|\n|    max|魔戒首部曲：魔戒現身|\n+-------+--------------------+\n\nres6: Seq[Unit] \u003d List(())\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://zeppelin:4040/jobs/job?id\u003d33"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1572298110787_580138552",
      "id": "20191028-212830_1786304880",
      "dateCreated": "2019-10-28 21:28:30.787",
      "dateStarted": "2019-10-29 23:50:40.649",
      "dateFinished": "2019-10-29 23:50:41.338",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### hint: Use the min value in the `average_rating` to clean the books.csv\nOr just import `books_clean.csv` and skip this whole process.",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:41.350",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003ehint: Use the min value in the \u003ccode\u003eaverage_rating\u003c/code\u003e to clean the books.csv\u003c/h3\u003e\n\u003cp\u003eOr just import \u003ccode\u003ebooks_clean.csv\u003c/code\u003e and skip this whole process.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572302132946_-2065061791",
      "id": "20191028-223532_578155414",
      "dateCreated": "2019-10-28 22:35:32.946",
      "dateStarted": "2019-10-29 23:50:41.433",
      "dateFinished": "2019-10-29 23:50:41.457",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Import the Clean Books Dataset",
      "text": "%spark\nval forSchemaDF \u003d spark.read\n    .format(\"csv\")\n    .option(\"delimiter\",\",\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(\"file:///odsc/books_clean.csv\")\n    .toDF(\"bookID\",\"title\",\"authors\",\"average_rating\",\"isbn\",\"isbn13\",\"language_code\",\"num_pages\",\"ratings_count\",\"text_reviews_count\")\nforSchemaDF.printSchema\nval schemaDDL \u003d forSchemaDF.schema.toDDL\n\nval correctSchema \u003d StructType.fromDDL(schemaDDL)\n\nval df \u003d spark.read\n    .format(\"csv\")\n    .option(\"delimiter\",\",\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"false\")\n    .schema(correctSchema)\n    .load(\"file:///odsc/books_clean.csv\")\n    .toDF(\"bookID\",\"title\",\"authors\",\"average_rating\",\"isbn\",\"isbn13\",\"language_code\",\"num_pages\",\"ratings_count\",\"text_reviews_count\")\n\ndf.createOrReplaceTempView(\"books\")",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:41.530",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- bookID: integer (nullable \u003d true)\n |-- title: string (nullable \u003d true)\n |-- authors: string (nullable \u003d true)\n |-- average_rating: double (nullable \u003d true)\n |-- isbn: string (nullable \u003d true)\n |-- isbn13: long (nullable \u003d true)\n |-- language_code: string (nullable \u003d true)\n |-- num_pages: integer (nullable \u003d true)\n |-- ratings_count: integer (nullable \u003d true)\n |-- text_reviews_count: integer (nullable \u003d true)\n\nforSchemaDF: org.apache.spark.sql.DataFrame \u003d [bookID: int, title: string ... 8 more fields]\nschemaDDL: String \u003d `bookID` INT,`title` STRING,`authors` STRING,`average_rating` DOUBLE,`isbn` STRING,`isbn13` BIGINT,`language_code` STRING,`num_pages` INT,`ratings_count` INT,`text_reviews_count` INT\ncorrectSchema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(bookID,IntegerType,true), StructField(title,StringType,true), StructField(authors,StringType,true), StructField(average_rating,DoubleType,true), StructField(isbn,StringType,true), StructField(isbn13,LongType,true), StructField(language_code,StringType,true), StructField(num_pages,IntegerType,true), StructField(ratings_count,IntegerType,true), StructField(text_reviews_count,IntegerType,true))\ndf: org.apache.spark.sql.Dat..."
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://zeppelin:4040/jobs/job?id\u003d34",
            "http://zeppelin:4040/jobs/job?id\u003d35"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1572297777757_879012840",
      "id": "20191028-212257_1356527838",
      "dateCreated": "2019-10-28 21:22:57.757",
      "dateStarted": "2019-10-29 23:50:41.574",
      "dateFinished": "2019-10-29 23:50:42.217",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\ndef calculateColumnAggregates(df: DataFrame) \u003d {\n  val dfs \u003d df.schema.map { r \u003d\u003e\n    val name \u003d r.name\n    val colData \u003d df.select(col(name)).where(col(name).isNotNull)\n    colData.agg(\n        lit(name) as \"colname\",\n        count(name) as \"count\",\n        min(name) as \"min\",\n        avg(name) as \"avg\",\n        stddev_pop(name) as \"sd_pop\",\n        stddev_samp(name) as \"sd_sample\",\n        max(name) as \"max\"\n    ).toDF(\"column\", \"count\", \"min\", \"avg\", \"sd_pop\", \"sd_sample\", \"max\")\n  }\n  val toFold \u003d dfs.splitAt(1)\n  toFold._2.fold(toFold._1.head)(_.union(_)).toDF(\"column\", \"count\", \"min\", \"avg\", \"sd_pop\", \"sd_sample\", \"max\")\n}",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:42.278",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "calculateColumnAggregates: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572303186880_135092792",
      "id": "20191028-225306_685360575",
      "dateCreated": "2019-10-28 22:53:06.881",
      "dateStarted": "2019-10-29 23:50:42.321",
      "dateFinished": "2019-10-29 23:50:42.777",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nval transformed \u003d df.transform(calculateColumnAggregates)",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:42.822",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "transformed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [column: string, count: bigint ... 5 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572302959192_-1180295937",
      "id": "20191028-224919_875811623",
      "dateCreated": "2019-10-28 22:49:19.192",
      "dateStarted": "2019-10-29 23:50:42.877",
      "dateFinished": "2019-10-29 23:50:43.415",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\ntransformed.show",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:43.481",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|            column|count|                 min|                 avg|              sd_pop|           sd_sample|                 max|\n+------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|            bookID|13719|                   1|  22160.579269626065|  13699.104049046236|  13699.603351218657|               47709|\n|             title|13719|\"A\" Is for Abduct...|              1900.8|  101.89877329978023|  113.92629196107458|魔戒首部曲：魔戒現身|\n|           authors|13719|A.B. Yehoshua-Hil...|                null|                null|                null|          Zoë Heller|\n|    average_rating|13719|                 0.0|  3.9302645965449154| 0.35941851738651354| 0.35943161739990825|                 5.0|\n|              isbn|13719|          000100039X|1.0400414419183592E9|1.5970322791529374E9|1.5970961336252508E9|          9998691567|\n|            isbn13|13719|          8987059752|9.764023529842604E12|3.986808723515777...|3.986954033913417...|       9790007672386|\n|     language_code|13719|                 ale|                null|                null|                null|                 zho|\n|         num_pages|13719|                   0|   342.3807128799475|  252.60968004206282|  252.61888710856644|                6576|\n|     ratings_count|13719|                   0|   17759.09126029594|  112933.00608063104|  112937.12223998511|             5629932|\n|text_reviews_count|13719|                   0|   533.4399008674102|  2528.4736441251484|  2528.5658014209025|               93619|\n+------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://zeppelin:4040/jobs/job?id\u003d36",
            "http://zeppelin:4040/jobs/job?id\u003d37",
            "http://zeppelin:4040/jobs/job?id\u003d38"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1572303946279_1991130333",
      "id": "20191028-230546_1200964219",
      "dateCreated": "2019-10-28 23:05:46.280",
      "dateStarted": "2019-10-29 23:50:43.519",
      "dateFinished": "2019-10-29 23:50:45.901",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Now that the Data is more or less fixed.\n1. Save the csv. For this example we are using Parquet",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:45.931",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eNow that the Data is more or less fixed.\u003c/h3\u003e\n\u003col\u003e\n  \u003cli\u003eSave the csv. For this example we are using Parquet\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572304947316_1571430230",
      "id": "20191028-232227_267452647",
      "dateCreated": "2019-10-28 23:22:27.316",
      "dateStarted": "2019-10-29 23:50:45.999",
      "dateFinished": "2019-10-29 23:50:46.008",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\ndf.write.mode(\"overwrite\").parquet(\"/odsc/books_clean.parquet\")",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:46.095",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://zeppelin:4040/jobs/job?id\u003d39"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1572303961899_1255373251",
      "id": "20191028-230601_1825955162",
      "dateCreated": "2019-10-28 23:06:01.899",
      "dateStarted": "2019-10-29 23:50:46.157",
      "dateFinished": "2019-10-29 23:50:48.260",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### What we just learned\n1. How to create functions to find missing and distinct values\n2. How to generate column statistics using the `DataFrame.agg` function\n3. How to look for misimported data and fix it (Data Cleaning)\n4. How to write these efforts back to disk. `df.write`\n",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:50:48.267",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eWhat we just learned\u003c/h3\u003e\n\u003col\u003e\n  \u003cli\u003eHow to create functions to find missing and distinct values\u003c/li\u003e\n  \u003cli\u003eHow to generate column statistics using the \u003ccode\u003eDataFrame.agg\u003c/code\u003e function\u003c/li\u003e\n  \u003cli\u003eHow to look for misimported data and fix it (Data Cleaning)\u003c/li\u003e\n  \u003cli\u003eHow to write these efforts back to disk. \u003ccode\u003edf.write\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572305041874_909767142",
      "id": "20191028-232401_1826115568",
      "dateCreated": "2019-10-28 23:24:01.874",
      "dateStarted": "2019-10-29 23:50:48.316",
      "dateFinished": "2019-10-29 23:50:48.330",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Up Next\n1. Learn how to use Exploratory Data Analysis techniques to find patterns in the data\n2. Use `Aggregations` and `Percentile` statistics to explore the data\n\nNext: [3-Spark-EDA](http://localhost:8080/#/notebook/2ESU4XURC)",
      "user": "anonymous",
      "dateUpdated": "2019-10-29 23:53:11.640",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eUp Next\u003c/h3\u003e\n\u003col\u003e\n  \u003cli\u003eLearn how to use Exploratory Data Analysis techniques to find patterns in the data\u003c/li\u003e\n  \u003cli\u003eUse \u003ccode\u003eAggregations\u003c/code\u003e and \u003ccode\u003ePercentile\u003c/code\u003e statistics to explore the data\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNext: \u003ca href\u003d\"http://localhost:8080/#/notebook/2ESU4XURC\"\u003e3-Spark-EDA\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1572305314940_429984409",
      "id": "20191028-232834_1961855414",
      "dateCreated": "2019-10-28 23:28:34.940",
      "dateStarted": "2019-10-29 23:53:11.641",
      "dateFinished": "2019-10-29 23:53:11.651",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "ODSC/2-SparkIntro-ColumnStats",
  "id": "2ET3A2ZBC",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}